2025-09-18 14:31:11,751 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 14:31:16,394 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 14:31:17,481 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 14:31:17,482 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 14:31:18,892 - app - INFO - 正在启动FastAPI应用...
2025-09-18 14:31:18,892 - app - INFO - 正在加载微调后的模型...
2025-09-18 14:31:18,893 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 14:32:00,239 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 14:32:21,289 - inference - INFO - Model loaded successfully
2025-09-18 14:32:21,289 - app - INFO - 模型加载成功！
2025-09-18 14:34:44,843 - app - INFO - 正在加载微调后的模型...
2025-09-18 14:34:44,843 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 14:35:26,268 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 14:35:58,782 - inference - INFO - Model loaded successfully
2025-09-18 14:35:58,797 - app - INFO - 模型加载成功！
2025-09-18 14:35:58,798 - app - INFO - 正在加载模型: initial_model
2025-09-18 14:35:58,799 - inference - INFO - Loading model from ./outputs\initial_model
2025-09-18 14:36:51,087 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 14:37:12,305 - inference - INFO - Model loaded successfully
2025-09-18 14:39:48,934 - app - INFO - 正在加载模型: checkpoint-630
2025-09-18 14:39:48,934 - inference - INFO - Loading model from ./outputs\checkpoint-630
2025-09-18 14:40:41,236 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 14:41:02,571 - inference - INFO - Model loaded successfully
2025-09-18 16:39:32,084 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 16:39:36,820 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 16:39:37,068 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 16:39:37,068 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 16:39:37,366 - app - INFO - 正在启动FastAPI应用...
2025-09-18 16:39:37,366 - app - INFO - 正在加载微调后的模型...
2025-09-18 16:39:37,367 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 16:40:40,786 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 16:41:01,803 - inference - INFO - Model loaded successfully
2025-09-18 16:41:01,803 - app - INFO - 模型加载成功！
2025-09-18 18:18:47,679 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 18:18:55,846 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 18:18:56,287 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 18:18:56,287 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 18:18:56,847 - app - INFO - 正在启动FastAPI应用...
2025-09-18 18:18:56,847 - app - INFO - 正在加载微调后的模型...
2025-09-18 18:18:56,848 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 18:18:57,095 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 18:18:58,834 - inference - INFO - Model loaded successfully
2025-09-18 18:18:58,834 - app - INFO - 模型加载成功！
2025-09-18 19:58:16,305 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 19:58:35,993 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 19:58:36,999 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 19:58:36,999 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 19:58:38,324 - app - INFO - 正在启动FastAPI应用...
2025-09-18 19:58:38,324 - app - INFO - 正在加载微调后的模型...
2025-09-18 19:58:38,324 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 19:58:42,133 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 19:58:45,196 - inference - INFO - Model loaded successfully
2025-09-18 19:58:45,196 - app - INFO - 模型加载成功！
2025-09-18 20:34:20,945 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 20:34:38,658 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 20:34:39,549 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 20:34:39,550 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 20:34:40,825 - app - INFO - 正在启动FastAPI应用...
2025-09-18 20:34:40,826 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 20:36:58,462 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 20:37:03,679 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 20:37:03,999 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 20:37:03,999 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 20:37:04,359 - app - INFO - 正在启动FastAPI应用...
2025-09-18 20:37:04,359 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 20:43:31,172 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 20:43:35,700 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 20:43:35,964 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 20:43:35,966 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 20:43:36,295 - app - INFO - 正在启动FastAPI应用...
2025-09-18 20:43:36,295 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 20:43:43,130 - train - INFO - Preparing training data...
2025-09-18 20:43:43,130 - model_utils - INFO - Loading tokenizer from Qwen/Qwen3-0.6B
2025-09-18 20:43:43,130 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 20:43:44,678 - model_utils - INFO - Tokenizer loaded. Vocab size: 151669
2025-09-18 20:43:44,680 - data_processor - INFO - Loaded 1674 conversations from ./datasets\LCCC-base_train.json
2025-09-18 20:43:44,681 - data_processor - INFO - Processed 1674 conversations from ./datasets\LCCC-base_train.json
2025-09-18 20:43:46,931 - data_processor - INFO - Training dataset: 1674 samples
2025-09-18 20:43:46,931 - data_processor - INFO - Loaded 207 conversations from ./datasets\LCCC-base_valid.json
2025-09-18 20:43:46,931 - data_processor - INFO - Processed 207 conversations from ./datasets\LCCC-base_valid.json
2025-09-18 20:43:48,748 - data_processor - INFO - Validation dataset: 207 samples
2025-09-18 20:43:48,749 - data_processor - INFO - Loaded 213 conversations from ./datasets\LCCC-base_test.json
2025-09-18 20:43:48,749 - data_processor - INFO - Processed 213 conversations from ./datasets\LCCC-base_test.json
2025-09-18 20:43:50,551 - data_processor - INFO - Test dataset: 213 samples
2025-09-18 20:43:50,551 - train - INFO - Setting up model...
2025-09-18 20:43:50,551 - model_utils - INFO - Loading model from Qwen/Qwen3-0.6B
2025-09-18 20:43:51,122 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 20:43:52,866 - model_utils - INFO - Model loaded successfully
2025-09-18 20:43:52,877 - train - INFO - Resized token embeddings to 151669
2025-09-18 20:43:52,877 - model_utils - INFO - Setting up PEFT model with LoRA
2025-09-18 20:43:53,240 - model_utils - INFO - Model parameters: 385,668,096
2025-09-18 20:43:53,240 - model_utils - INFO - Parameter memory: 841.21 MB
2025-09-18 20:43:53,241 - model_utils - INFO - Buffer memory: 0.00 MB
2025-09-18 20:43:53,241 - model_utils - INFO - Total memory: 841.21 MB
2025-09-18 20:43:53,347 - app - INFO - Saving initial model state...
2025-09-18 20:43:55,714 - app - INFO - Starting training...
2025-09-18 20:44:10,998 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 20:44:15,278 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 20:44:15,525 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 20:44:15,525 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 20:44:15,842 - app - INFO - 正在启动FastAPI应用...
2025-09-18 20:44:15,842 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 20:44:28,860 - train - INFO - Preparing training data...
2025-09-18 20:44:28,861 - model_utils - INFO - Loading tokenizer from Qwen/Qwen3-0.6B
2025-09-18 20:44:28,861 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 20:44:30,521 - model_utils - INFO - Tokenizer loaded. Vocab size: 151669
2025-09-18 20:44:30,523 - data_processor - INFO - Loaded 1674 conversations from ./datasets\LCCC-base_train.json
2025-09-18 20:44:30,525 - data_processor - INFO - Processed 1674 conversations from ./datasets\LCCC-base_train.json
2025-09-18 20:44:32,681 - data_processor - INFO - Training dataset: 1674 samples
2025-09-18 20:44:32,682 - data_processor - INFO - Loaded 207 conversations from ./datasets\LCCC-base_valid.json
2025-09-18 20:44:32,682 - data_processor - INFO - Processed 207 conversations from ./datasets\LCCC-base_valid.json
2025-09-18 20:44:34,477 - data_processor - INFO - Validation dataset: 207 samples
2025-09-18 20:44:34,478 - data_processor - INFO - Loaded 213 conversations from ./datasets\LCCC-base_test.json
2025-09-18 20:44:34,478 - data_processor - INFO - Processed 213 conversations from ./datasets\LCCC-base_test.json
2025-09-18 20:44:36,340 - data_processor - INFO - Test dataset: 213 samples
2025-09-18 20:44:36,340 - train - INFO - Setting up model...
2025-09-18 20:44:36,340 - model_utils - INFO - Loading model from Qwen/Qwen3-0.6B
2025-09-18 20:44:37,141 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 20:44:39,284 - model_utils - INFO - Model loaded successfully
2025-09-18 20:44:39,295 - train - INFO - Resized token embeddings to 151669
2025-09-18 20:44:39,295 - model_utils - INFO - Setting up PEFT model with LoRA
2025-09-18 20:44:39,684 - model_utils - INFO - Model parameters: 385,668,096
2025-09-18 20:44:39,684 - model_utils - INFO - Parameter memory: 841.21 MB
2025-09-18 20:44:39,685 - model_utils - INFO - Buffer memory: 0.00 MB
2025-09-18 20:44:39,685 - model_utils - INFO - Total memory: 841.21 MB
2025-09-18 20:44:39,742 - app - INFO - Saving initial model state...
2025-09-18 20:44:42,221 - app - INFO - Starting training...
2025-09-18 20:46:36,902 - app - INFO - Saving final model...
2025-09-18 20:46:39,432 - app - INFO - Training completed successfully!
2025-09-18 20:47:03,717 - app - INFO - 正在加载模型: final_model
2025-09-18 20:47:03,718 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 20:47:08,868 - app - ERROR - 模型加载失败: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151669, 1024]) from checkpoint, the shape in current model is torch.Size([151936, 1024]).
	size mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151669, 1024]) from checkpoint, the shape in current model is torch.Size([151936, 1024]).
2025-09-18 20:47:10,563 - app - INFO - 正在加载模型: final_model
2025-09-18 20:47:10,563 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 20:47:14,804 - app - ERROR - 模型加载失败: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151669, 1024]) from checkpoint, the shape in current model is torch.Size([151936, 1024]).
	size mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151669, 1024]) from checkpoint, the shape in current model is torch.Size([151936, 1024]).
2025-09-18 20:47:40,888 - train - INFO - Preparing training data...
2025-09-18 20:47:40,888 - model_utils - INFO - Loading tokenizer from Qwen/Qwen3-0.6B
2025-09-18 20:47:40,888 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 20:47:42,128 - model_utils - INFO - Tokenizer loaded. Vocab size: 151669
2025-09-18 20:47:42,132 - data_processor - INFO - Loaded 1674 conversations from ./datasets\LCCC-base_train.json
2025-09-18 20:47:42,134 - data_processor - INFO - Processed 1674 conversations from ./datasets\LCCC-base_train.json
2025-09-18 20:47:44,524 - data_processor - INFO - Training dataset: 1674 samples
2025-09-18 20:47:44,524 - data_processor - INFO - Loaded 207 conversations from ./datasets\LCCC-base_valid.json
2025-09-18 20:47:44,524 - data_processor - INFO - Processed 207 conversations from ./datasets\LCCC-base_valid.json
2025-09-18 20:47:46,515 - data_processor - INFO - Validation dataset: 207 samples
2025-09-18 20:47:46,516 - data_processor - INFO - Loaded 213 conversations from ./datasets\LCCC-base_test.json
2025-09-18 20:47:46,516 - data_processor - INFO - Processed 213 conversations from ./datasets\LCCC-base_test.json
2025-09-18 20:47:48,461 - data_processor - INFO - Test dataset: 213 samples
2025-09-18 20:47:48,461 - train - INFO - Setting up model...
2025-09-18 20:47:48,462 - model_utils - INFO - Loading model from Qwen/Qwen3-0.6B
2025-09-18 20:47:58,543 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 20:48:00,808 - model_utils - INFO - Model loaded successfully
2025-09-18 20:48:00,808 - train - INFO - Resized token embeddings to 151669
2025-09-18 20:48:00,810 - model_utils - INFO - Setting up PEFT model with LoRA
2025-09-18 20:48:01,280 - model_utils - INFO - Model parameters: 385,668,096
2025-09-18 20:48:01,280 - model_utils - INFO - Parameter memory: 841.21 MB
2025-09-18 20:48:01,280 - model_utils - INFO - Buffer memory: 0.00 MB
2025-09-18 20:48:01,280 - model_utils - INFO - Total memory: 841.21 MB
2025-09-18 20:48:01,341 - app - INFO - Saving initial model state...
2025-09-18 20:48:03,578 - app - INFO - Starting training...
2025-09-18 20:53:57,368 - app - INFO - Saving final model...
2025-09-18 20:53:59,915 - app - INFO - Training completed successfully!
2025-09-18 20:54:06,524 - app - INFO - 正在加载模型: final_model
2025-09-18 20:54:06,525 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 20:54:08,097 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 20:54:10,249 - app - ERROR - 模型加载失败: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151669, 1024]) from checkpoint, the shape in current model is torch.Size([151936, 1024]).
	size mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151669, 1024]) from checkpoint, the shape in current model is torch.Size([151936, 1024]).
2025-09-18 21:01:46,046 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 21:01:50,281 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 21:01:50,535 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 21:01:50,536 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 21:01:50,843 - app - INFO - 正在启动FastAPI应用...
2025-09-18 21:01:50,844 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 21:02:12,703 - __main__ - INFO - 正在启动Qwen3-0.6B QLoRA Web应用...
2025-09-18 21:02:17,100 - __main__ - INFO - 启动Web服务器: http://127.0.0.1:8000
2025-09-18 21:02:17,356 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-09-18 21:02:17,356 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2025-09-18 21:02:17,673 - app - INFO - 正在启动FastAPI应用...
2025-09-18 21:02:17,673 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 21:02:34,075 - train - INFO - Preparing training data...
2025-09-18 21:02:34,075 - model_utils - INFO - Loading tokenizer from Qwen/Qwen3-0.6B
2025-09-18 21:02:34,075 - app - WARNING - 未找到微调后的模型，请先进行训练
2025-09-18 21:02:35,151 - model_utils - INFO - Set unk_token to <unk>
2025-09-18 21:02:35,151 - model_utils - INFO - Tokenizer配置完成:
2025-09-18 21:02:35,151 - model_utils - INFO -   - 词汇表大小: 151669
2025-09-18 21:02:35,151 - model_utils - INFO -   - pad_token: <|endoftext|>
2025-09-18 21:02:35,151 - model_utils - INFO -   - eos_token: <|im_end|>
2025-09-18 21:02:35,151 - model_utils - INFO -   - unk_token: <unk>
2025-09-18 21:02:35,151 - model_utils - INFO -   - unk_token_id: 128244
2025-09-18 21:02:35,152 - train - INFO - Tokenizer配置:
2025-09-18 21:02:35,152 - train - INFO -   - 词汇表大小: 151669
2025-09-18 21:02:35,152 - train - INFO -   - pad_token: <|endoftext|>
2025-09-18 21:02:35,153 - train - INFO -   - eos_token: <|im_end|>
2025-09-18 21:02:35,153 - train - INFO -   - unk_token: <unk>
2025-09-18 21:02:35,154 - data_processor - INFO - Loaded 1674 conversations from ./datasets\LCCC-base_train.json
2025-09-18 21:02:35,156 - data_processor - INFO - Processed 1674 conversations from ./datasets\LCCC-base_train.json
2025-09-18 21:02:37,334 - data_processor - INFO - Training dataset: 1674 samples
2025-09-18 21:02:37,334 - data_processor - INFO - Loaded 207 conversations from ./datasets\LCCC-base_valid.json
2025-09-18 21:02:37,335 - data_processor - INFO - Processed 207 conversations from ./datasets\LCCC-base_valid.json
2025-09-18 21:02:39,095 - data_processor - INFO - Validation dataset: 207 samples
2025-09-18 21:02:39,096 - data_processor - INFO - Loaded 213 conversations from ./datasets\LCCC-base_test.json
2025-09-18 21:02:39,096 - data_processor - INFO - Processed 213 conversations from ./datasets\LCCC-base_test.json
2025-09-18 21:02:40,861 - data_processor - INFO - Test dataset: 213 samples
2025-09-18 21:02:40,861 - train - INFO - Setting up model...
2025-09-18 21:02:40,861 - model_utils - INFO - Loading model from Qwen/Qwen3-0.6B
2025-09-18 21:02:41,408 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 21:02:43,055 - model_utils - INFO - Model loaded successfully
2025-09-18 21:02:43,056 - train - INFO - 模型词汇表大小: 151936
2025-09-18 21:02:43,056 - train - INFO - Tokenizer词汇表大小: 151669
2025-09-18 21:02:43,056 - train - WARNING - 词汇表大小不匹配，但不调整模型大小
2025-09-18 21:02:43,056 - train - WARNING - 未知词汇将使用 unk_token: <unk>
2025-09-18 21:02:43,056 - model_utils - INFO - Setting up PEFT model with LoRA
2025-09-18 21:02:43,364 - app - INFO - Tokenizer vocab size: 151669
2025-09-18 21:02:43,364 - app - INFO - Using unk_token: <unk>
2025-09-18 21:02:43,374 - model_utils - INFO - Model parameters: 385,941,504
2025-09-18 21:02:43,374 - model_utils - INFO - Parameter memory: 842.25 MB
2025-09-18 21:02:43,374 - model_utils - INFO - Buffer memory: 0.00 MB
2025-09-18 21:02:43,374 - model_utils - INFO - Total memory: 842.25 MB
2025-09-18 21:02:43,433 - app - INFO - Saving initial model state...
2025-09-18 21:02:44,674 - app - INFO - Starting training...
2025-09-18 21:04:50,857 - app - INFO - Saving final model...
2025-09-18 21:04:52,760 - app - INFO - Training completed successfully!
2025-09-18 21:05:02,315 - app - INFO - 正在加载模型: final_model
2025-09-18 21:05:02,316 - inference - INFO - Loading model from ./outputs\final_model
2025-09-18 21:05:02,316 - inference - INFO - Trying to load tokenizer from ./outputs\final_model
2025-09-18 21:05:02,462 - inference - INFO - Successfully loaded tokenizer from ./outputs\final_model
2025-09-18 21:05:02,462 - inference - INFO - Tokenizer vocab size: 151669
2025-09-18 21:05:02,462 - inference - INFO - Tokenizer unk_token: <unk>
2025-09-18 21:05:02,462 - inference - INFO - Loading base model: Qwen/Qwen3-0.6B
2025-09-18 21:05:03,165 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-18 21:05:04,574 - inference - INFO - Base model vocab size: 151936
2025-09-18 21:05:04,575 - inference - WARNING - Tokenizer vocab size (151669) != model vocab size (151936)
2025-09-18 21:05:04,575 - inference - WARNING - This may cause issues. Consider using the original tokenizer.
2025-09-18 21:05:04,575 - inference - WARNING - Unknown tokens will be handled by unk_token if available.
2025-09-18 21:05:04,575 - inference - INFO - Loading PEFT weights from ./outputs\final_model
2025-09-18 21:05:04,877 - inference - INFO - PEFT model loaded successfully
2025-09-18 21:05:04,882 - inference - INFO - Model loaded successfully
